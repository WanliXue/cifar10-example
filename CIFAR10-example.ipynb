{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants describing the CIFAR-10 data set.\n",
    "IMAGE_SIZE = 24\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "\n",
    "# Program parameters\n",
    "f_batch_size = 128 #Number of images to process in a batch.\n",
    "f_use_fp16 = False #Train the model using fp16.\n",
    "f_max_steps = 100 #Number of batches to run.\n",
    "f_log_device_placement = False #Whether to log device placement.\n",
    "f_log_frequency = 10 #How often to log results to the console.\n",
    "f_eval_data = 'test' #Either 'test' or 'train_eval'.\n",
    "f_eval_interval_secs = 30 #How often to run the eval\n",
    "f_num_examples = 10000 #Number of examples to run.\n",
    "f_run_once = True #Whether to run eval only once.\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "# Paths\n",
    "f_data_dir = '../CIFAR10_data/cifar10_data' #CIFAR-10 data\n",
    "f_eval_dir = '../CIFAR10_data/cifar10_eval' #event logs\n",
    "f_train_dir = '../CIFAR10_data/cifar10_train' #event logs and checkpoint\n",
    "f_checkpoint_dir = '../CIFAR10_data/cifar10_train' #read model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cifar10(filename_queue):\n",
    "  \"\"\"Reads and parses examples from CIFAR10 data files.\n",
    "\n",
    "  Recommendation: if you want N-way read parallelism, call this function\n",
    "  N times.  This will give you N independent Readers reading different\n",
    "  files & positions within those files, which will give better mixing of\n",
    "  examples.\n",
    "\n",
    "  Args:\n",
    "    filename_queue: A queue of strings with the filenames to read from.\n",
    "\n",
    "  Returns:\n",
    "    An object representing a single example, with the following fields:\n",
    "      height: number of rows in the result (32)\n",
    "      width: number of columns in the result (32)\n",
    "      depth: number of color channels in the result (3)\n",
    "      key: a scalar string Tensor describing the filename & record number\n",
    "        for this example.\n",
    "      label: an int32 Tensor with the label in the range 0..9.\n",
    "      uint8image: a [height, width, depth] uint8 Tensor with the image data\n",
    "  \"\"\"\n",
    "\n",
    "  class CIFAR10Record(object):\n",
    "    pass\n",
    "  result = CIFAR10Record()\n",
    "\n",
    "  # Dimensions of the images in the CIFAR-10 dataset.\n",
    "  # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "  # input format.\n",
    "  label_bytes = 1  # 2 for CIFAR-100\n",
    "  result.height = 32\n",
    "  result.width = 32\n",
    "  result.depth = 3\n",
    "  image_bytes = result.height * result.width * result.depth\n",
    "  # Every record consists of a label followed by the image, with a\n",
    "  # fixed number of bytes for each.\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "\n",
    "  # Read a record, getting filenames from the filename_queue.  No\n",
    "  # header or footer in the CIFAR-10 format, so we leave header_bytes\n",
    "  # and footer_bytes at their default of 0.\n",
    "  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "  result.key, value = reader.read(filename_queue)\n",
    "\n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "  # The first bytes represent the label, which we convert from uint8->int32.\n",
    "  result.label = tf.cast(\n",
    "      tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "      tf.strided_slice(record_bytes, [label_bytes],\n",
    "                       [label_bytes + image_bytes]),\n",
    "      [result.depth, result.height, result.width])\n",
    "  # Convert from [depth, height, width] to [height, width, depth].\n",
    "  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "def _generate_image_and_label_batch(image, label, min_queue_examples,\n",
    "                                    batch_size, shuffle):\n",
    "  \"\"\"Construct a queued batch of images and labels.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "    label: 1-D Tensor of type.int32\n",
    "    min_queue_examples: int32, minimum number of samples to retain\n",
    "      in the queue that provides of batches of examples.\n",
    "    batch_size: Number of images per batch.\n",
    "    shuffle: boolean indicating whether to use a shuffling queue.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  \"\"\"\n",
    "  # Create a queue that shuffles the examples, and then\n",
    "  # read 'batch_size' images + labels from the example queue.\n",
    "  num_preprocess_threads = 16\n",
    "  if shuffle:\n",
    "    images, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples)\n",
    "  else:\n",
    "    images, label_batch = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "  # Display the training images in the visualizer.\n",
    "  tf.summary.image('images', images)\n",
    "\n",
    "  return images, tf.reshape(label_batch, [batch_size])\n",
    "\n",
    "\n",
    "def distorted_inputs2(data_dir, batch_size):\n",
    "  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n",
    "\n",
    "  Args:\n",
    "    data_dir: Path to the CIFAR-10 data directory.\n",
    "    batch_size: Number of images per batch.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  \"\"\"\n",
    "  filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "               for i in range(1, 6)]\n",
    "  for f in filenames:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "  # Create a queue that produces the filenames to read.\n",
    "  filename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "  # Read examples from files in the filename queue.\n",
    "  read_input = read_cifar10(filename_queue)\n",
    "  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "  height = IMAGE_SIZE\n",
    "  width = IMAGE_SIZE\n",
    "\n",
    "  # Image processing for training the network. Note the many random\n",
    "  # distortions applied to the image.\n",
    "\n",
    "  # Randomly crop a [height, width] section of the image.\n",
    "  distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n",
    "\n",
    "  # Randomly flip the image horizontally.\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "  # Because these operations are not commutative, consider randomizing\n",
    "  # the order their operation.\n",
    "  # NOTE: since per_image_standardization zeros the mean and makes\n",
    "  # the stddev unit, this likely has no effect see tensorflow#1458.\n",
    "  distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "  distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "  # Set the shapes of tensors.\n",
    "  float_image.set_shape([height, width, 3])\n",
    "  read_input.label.set_shape([1])\n",
    "\n",
    "  # Ensure that the random shuffling has good mixing properties.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n",
    "                           min_fraction_of_examples_in_queue)\n",
    "  print ('Filling queue with %d CIFAR images before starting to train. '\n",
    "         'This will take a few minutes.' % min_queue_examples)\n",
    "\n",
    "  # Generate a batch of images and labels by building up a queue of examples.\n",
    "  return _generate_image_and_label_batch(float_image, read_input.label,\n",
    "                                         min_queue_examples, batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "\n",
    "def inputs2(eval_data, data_dir, batch_size):\n",
    "  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "\n",
    "  Args:\n",
    "    eval_data: bool, indicating if one should use the train or eval data set.\n",
    "    data_dir: Path to the CIFAR-10 data directory.\n",
    "    batch_size: Number of images per batch.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "  \"\"\"\n",
    "  if not eval_data:\n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "                 for i in range(1, 6)]\n",
    "    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "  else:\n",
    "    filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "  for f in filenames:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "  # Create a queue that produces the filenames to read.\n",
    "  filename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "  # Read examples from files in the filename queue.\n",
    "  read_input = read_cifar10(filename_queue)\n",
    "  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "  height = IMAGE_SIZE\n",
    "  width = IMAGE_SIZE\n",
    "\n",
    "  # Image processing for evaluation.\n",
    "  # Crop the central [height, width] of the image.\n",
    "  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         height, width)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "  # Set the shapes of tensors.\n",
    "  float_image.set_shape([height, width, 3])\n",
    "  read_input.label.set_shape([1])\n",
    "\n",
    "  # Ensure that the random shuffling has good mixing properties.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(num_examples_per_epoch *\n",
    "                           min_fraction_of_examples_in_queue)\n",
    "\n",
    "  # Generate a batch of images and labels by building up a queue of examples.\n",
    "  return _generate_image_and_label_batch(float_image, read_input.label,\n",
    "                                         min_queue_examples, batch_size,\n",
    "                                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measures the sparsity of activations.\n",
    "\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "  # session. This helps the clarity of presentation on tensorboard.\n",
    "  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "  tf.summary.histogram(tensor_name + '/activations', x)\n",
    "  tf.summary.scalar(tensor_name + '/sparsity',\n",
    "                                       tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float16 if f_use_fp16 else tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float16 if f_use_fp16 else tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var\n",
    "\n",
    "\n",
    "def distorted_inputs():\n",
    "  \"\"\"Construct distorted input for CIFAR training using the Reader ops.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If no data_dir\n",
    "  \"\"\"\n",
    "  if not f_data_dir:\n",
    "    raise ValueError('Please supply a data_dir')\n",
    "  data_dir = os.path.join(f_data_dir, 'cifar-10-batches-bin')\n",
    "  images, labels = distorted_inputs2(data_dir=data_dir,\n",
    "                                                  batch_size=f_batch_size)\n",
    "  if f_use_fp16:\n",
    "    images = tf.cast(images, tf.float16)\n",
    "    labels = tf.cast(labels, tf.float16)\n",
    "  return images, labels\n",
    "\n",
    "\n",
    "def inputs(eval_data):\n",
    "  \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "\n",
    "  Args:\n",
    "    eval_data: bool, indicating if one should use the train or eval data set.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "    labels: Labels. 1D tensor of [batch_size] size.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If no data_dir\n",
    "  \"\"\"\n",
    "  if not f_data_dir:\n",
    "    raise ValueError('Please supply a data_dir')\n",
    "  data_dir = os.path.join(f_data_dir, 'cifar-10-batches-bin')\n",
    "  images, labels = inputs2(eval_data=eval_data,\n",
    "                                        data_dir=data_dir,\n",
    "                                        batch_size=f_batch_size)\n",
    "  if f_use_fp16:\n",
    "    images = tf.cast(images, tf.float16)\n",
    "    labels = tf.cast(labels, tf.float16)\n",
    "  return images, labels\n",
    "\n",
    "\n",
    "def inference(images):\n",
    "  \"\"\"Build the CIFAR-10 model.\n",
    "\n",
    "  Args:\n",
    "    images: Images returned from distorted_inputs() or inputs().\n",
    "\n",
    "  Returns:\n",
    "    Logits.\n",
    "  \"\"\"\n",
    "  # We instantiate all variables using tf.get_variable() instead of\n",
    "  # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "  # If we only ran this model on a single GPU, we could simplify this function\n",
    "  # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "  #\n",
    "  # conv1\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 3, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "\n",
    "  # pool1\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "  # norm1\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "  # conv2\n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 64, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "  # norm2\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "  # pool2\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "  # local3\n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    # Move everything into depth so we can perform a single matrix multiply.\n",
    "    reshape = tf.reshape(pool2, [f_batch_size, -1])\n",
    "    dim = reshape.get_shape()[1].value\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  # local4\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  # linear layer(WX + b),\n",
    "  # We don't apply softmax here because\n",
    "  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "  # and performs the softmax internally for efficiency.\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                              tf.constant_initializer(0.0))\n",
    "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(softmax_linear)\n",
    "\n",
    "  return softmax_linear\n",
    "\n",
    "\n",
    "def loss1(logits, labels):\n",
    "  \"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "  Add summary for \"Loss\" and \"Loss/avg\".\n",
    "  Args:\n",
    "    logits: Logits from inference().\n",
    "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "\n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  # Calculate the average cross entropy loss across the batch.\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "  # decay terms (L2 loss).\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "  \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "\n",
    "  Generates moving average for all losses and associated summaries for\n",
    "  visualizing the performance of the network.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "  Returns:\n",
    "    loss_averages_op: op for generating moving averages of losses.\n",
    "  \"\"\"\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "  losses = tf.get_collection('losses')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "  # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "  # same for the averaged version of the losses.\n",
    "  for l in losses + [total_loss]:\n",
    "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "    # as the original loss name.\n",
    "    tf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "    tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "  return loss_averages_op\n",
    "\n",
    "\n",
    "def train1(total_loss, global_step):\n",
    "  \"\"\"Train CIFAR-10 model.\n",
    "\n",
    "  Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "  Returns:\n",
    "    train_op: op for training.\n",
    "  \"\"\"\n",
    "  # Variables that affect learning rate.\n",
    "  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / f_batch_size\n",
    "  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "  # Decay the learning rate exponentially based on the number of steps.\n",
    "  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "  tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "  # Generate moving averages of all losses and associated summaries.\n",
    "  loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "  # Compute gradients.\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "  # Apply gradients.\n",
    "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "  # Add histograms for trainable variables.\n",
    "  for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "  # Add histograms for gradients.\n",
    "  for grad, var in grads:\n",
    "    if grad is not None:\n",
    "      tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "  variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "\n",
    "  return train_op\n",
    "\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "  dest_directory = f_data_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "          float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train2():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
    "    # GPU and resulting in a slow down.\n",
    "    with tf.device('/cpu:0'):\n",
    "      images, labels = distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = loss1(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = train1(loss, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "        self._start_time = time.time()\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        if self._step % f_log_frequency == 0:\n",
    "          current_time = time.time()\n",
    "          duration = current_time - self._start_time\n",
    "          self._start_time = current_time\n",
    "\n",
    "          loss_value = run_values.results\n",
    "          examples_per_sec = f_log_frequency * f_batch_size / duration\n",
    "          sec_per_batch = float(duration / f_log_frequency)\n",
    "\n",
    "          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, loss_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "        checkpoint_dir=f_train_dir,\n",
    "        hooks=[tf.train.StopAtStepHook(last_step=f_max_steps),\n",
    "               tf.train.NanTensorHook(loss),\n",
    "               _LoggerHook()],\n",
    "        config=tf.ConfigProto(\n",
    "            log_device_placement=f_log_device_placement)) as mon_sess:\n",
    "      while not mon_sess.should_stop():\n",
    "        mon_sess.run(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n",
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../CIFAR10_data/cifar10_train\\model.ckpt-105\n",
      "INFO:tensorflow:Saving checkpoints for 106 into ../CIFAR10_data/cifar10_train\\model.ckpt.\n",
      "2017-08-02 15:15:22.297988: step 0, loss = 4.03 (188.6 examples/sec; 0.679 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_extract()\n",
    "#if tf.gfile.Exists(f_train_dir):\n",
    " #   tf.gfile.DeleteRecursively(f_train_dir)\n",
    "tf.gfile.MakeDirs(f_train_dir)\n",
    "train2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_once(saver, summary_writer, top_k_op, summary_op):\n",
    "  \"\"\"Run Eval once.\n",
    "\n",
    "  Args:\n",
    "    saver: Saver.\n",
    "    summary_writer: Summary writer.\n",
    "    top_k_op: Top K op.\n",
    "    summary_op: Summary op.\n",
    "  \"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(f_checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      # Restores from checkpoint\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "      # Assuming model_checkpoint_path looks something like:\n",
    "      #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "      # extract global_step from it.\n",
    "      global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    else:\n",
    "      print('No checkpoint file found')\n",
    "      return\n",
    "\n",
    "    # Start the queue runners.\n",
    "    coord = tf.train.Coordinator()\n",
    "    try:\n",
    "      threads = []\n",
    "      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                         start=True))\n",
    "\n",
    "      num_iter = int(math.ceil(f_num_examples / f_batch_size))\n",
    "      true_count = 0  # Counts the number of correct predictions.\n",
    "      total_sample_count = num_iter * f_batch_size\n",
    "      step = 0\n",
    "      while step < num_iter and not coord.should_stop():\n",
    "        predictions = sess.run([top_k_op])\n",
    "        true_count += np.sum(predictions)\n",
    "        step += 1\n",
    "\n",
    "      # Compute precision @ 1.\n",
    "      precision = true_count / total_sample_count\n",
    "      print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "      summary = tf.Summary()\n",
    "      summary.ParseFromString(sess.run(summary_op))\n",
    "      summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "      summary_writer.add_summary(summary, global_step)\n",
    "    except Exception as e:  # pylint: disable=broad-except\n",
    "      coord.request_stop(e)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=10)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "  \"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default() as g:\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    eval_data = f_eval_data == 'test'\n",
    "    images, labels = inputs(eval_data=eval_data)\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Calculate predictions.\n",
    "    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "    # Restore the moving average version of the learned variables for eval.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY)\n",
    "    variables_to_restore = variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(f_eval_dir, g)\n",
    "\n",
    "    while True:\n",
    "      eval_once(saver, summary_writer, top_k_op, summary_op)\n",
    "      if f_run_once:\n",
    "        break\n",
    "      time.sleep(f_eval_interval_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../CIFAR10_data/cifar10_train\\model.ckpt-106\n",
      "2017-08-02 15:15:40.018503: precision @ 1 = 0.365\n"
     ]
    }
   ],
   "source": [
    "#if tf.gfile.Exists(f_eval_dir):\n",
    "#    tf.gfile.DeleteRecursively(f_eval_dir)\n",
    "tf.gfile.MakeDirs(f_eval_dir)\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "custom_dir = \"../CIFAR10_data/cifar10_custom/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom = Image.open(custom_dir+\"1.jpg\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_data = np.array(custom.getdata()).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_data = 255-cust_data.reshape(32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBQFvxCJouAXIlEU/EIkioJfiET5//osRoNwS281AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb409588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.clf\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "cust_data2 = cust_data.copy()\n",
    "cust_data2[:,:,0] = cust_data[:,:,0]\n",
    "cust_data2[:,:,1] = cust_data[:,:,1]\n",
    "cust_data2[:,:,2] = cust_data[:,:,2]\n",
    "ax.imshow(cust_data2)\n",
    "print(cust_data.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Record(object):\n",
    "    pass\n",
    "result = CIFAR10Record()\n",
    "\n",
    "# Dimensions of the images in the CIFAR-10 dataset.\n",
    "# See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "# input format.\n",
    "label_bytes = 1  # 2 for CIFAR-100\n",
    "result.height = 32\n",
    "result.width = 32\n",
    "result.depth = 3\n",
    "image_bytes = result.height * result.width * result.depth\n",
    "\n",
    "# Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "record_bytes = tf.stack(cust_data)\n",
    "\n",
    "# The first bytes represent the label, which we convert from uint8->int32.\n",
    "result.label = 3\n",
    "\n",
    "# The remaining bytes after the label represent the image, which we reshape\n",
    "# from [depth * height * width] to [depth, height, width].\n",
    "depth_major = tf.reshape(record_bytes,\n",
    "    [result.depth, result.height, result.width])\n",
    "# Convert from [depth, height, width] to [height, width, depth].\n",
    "result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "height = IMAGE_SIZE\n",
    "width = IMAGE_SIZE\n",
    "\n",
    "reshaped_image = tf.cast(result.uint8image, tf.float32)\n",
    "\n",
    "# Image processing for evaluation.\n",
    "# Crop the central [height, width] of the image.\n",
    "resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         height, width)\n",
    "\n",
    "# Subtract off the mean and divide by the variance of the pixels.\n",
    "float_image = tf.image.per_image_standardization(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_image,label = _generate_image_and_label_batch(float_image, 2, 1,\n",
    "                                    1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-acf217ea19c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Restores from checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1159\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1162\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m       self.saver_def = self._builder.build(\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  ckpt = tf.train.get_checkpoint_state(f_checkpoint_dir)\n",
    "  if ckpt and ckpt.model_checkpoint_path:\n",
    "    # Restores from checkpoint\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "  with tf.variable_scope('conv1', reuse=True) as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 3, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(custom_image, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "  # conv2\n",
    "  with tf.variable_scope('conv2',reuse=True) as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 64, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "  # local3\n",
    "  with tf.variable_scope('local3',reuse=True) as scope:\n",
    "    # Move everything into depth so we can perform a single matrix multiply.\n",
    "    reshape = tf.reshape(pool2, [f_batch_size, -1])\n",
    "    dim = reshape.get_shape()[1].value\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  # local4\n",
    "  with tf.variable_scope('local4',reuse=True) as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  # linear layer(WX + b),\n",
    "  # We don't apply softmax here because\n",
    "  # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "  # and performs the softmax internally for efficiency.\n",
    "  with tf.variable_scope('softmax_linear',reuse=True) as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                              tf.constant_initializer(0.0))\n",
    "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(softmax_linear)\n",
    "    softmax_linear.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
